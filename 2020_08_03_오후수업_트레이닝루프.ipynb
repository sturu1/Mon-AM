{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020_08_03_오후수업_트레이닝루프.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0M/vySPq6mOdHwAzomj2B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sturu1/Mon-AM/blob/master/2020_08_03_%EC%98%A4%ED%9B%84%EC%88%98%EC%97%85_%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%8B%9D%EB%A3%A8%ED%94%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK0SgllHNCAW",
        "colab_type": "text"
      },
      "source": [
        "트레이닝 루프"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NjkXFnqNB5u",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_TQrsPeM1D0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlS0iAu3P4ec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch_size = 64\n",
        "# (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# x_train = np.reshape(x_train, (-1, 784))\n",
        "# x_test = np.reshape(x_train, (-1, 784))\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "# train_dataset = train_dataset.shuffle(buffer_size = 1024).batch(batch_size)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg11K4q0NOql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#모델정의\n",
        "inputs = keras.Input(shape = (784, ), name = \"digits\")\n",
        "x = layers.Dense(64, activation = \"relu\", name = \"dense_1\")(inputs)\n",
        "x = layers.Dense(64, activation = \"relu\", name = \"dense_2\")(x)\n",
        "outputs = layers.Dense(10, name = \"prediotions\")(x)\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "# 최적화 인스턴스\n",
        "optimizer = keras.optimizers.SGD(learning_rate = 1e-3)\n",
        "\n",
        "#손실함수 정의\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "\n",
        "#정확도 측정 인스턴스\n",
        "train_acc_metric = keras.metrics.SparseCategoricalCrossentropy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalCrossentropy()\n",
        "\n",
        "#훈련데이터셋 준비\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = np.reshape(x_train, (-1, 784))\n",
        "x_test = np.reshape(x_train, (-1, 784))\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size = 1024).batch(batch_size)\n",
        "\n",
        "#10000개 데이터셋은 검증데이터셋으로 설정\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(64)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8f460UsSEsm",
        "colab_type": "text"
      },
      "source": [
        "# @tf.function 데코레이션을 붙이면 학습속도가 빨라진다.\n",
        "\n",
        " gradient를 구해서 가중치를 업데이트하는 부분이 학습의 핵심이라 볼 수 잇고, 그 부분만 함수로 따로 뺸 다음 @tf.function을 붙여주면 빨라진다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4kVdvZERM26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(x, training = True)\n",
        "    loss_value = loss_fn(y, logits)\n",
        "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "  train_acc_metric.update_state(y, logits)\n",
        "  return loss_value"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF9-eS5EPdB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(x, y):\n",
        "  val_logits = model(x, training = False)\n",
        "  val_acc_metric.update_state(y, val_logits)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwpiBeGdSru2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "d54d492c-f828-41a1-d826-b3fe1c0c9f81"
      },
      "source": [
        "import time\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "  print(\"\\n%d번째 epoch 시작\" % (epoch, ))\n",
        "  start_time = time.time()\n",
        "  #\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "    loss_value = train_step(x_batch_train, y_batch_train)\n",
        "    #\n",
        "    if step % 200 == 0:\n",
        "      print(\"Training loss (for one batch) at step %d 번째 스텝에서 배치별 훈련데이터 손실: %.4f\" % (step, float(loss_value)))\n",
        "      print(\"현재까지 학습한 데이터 개수: %d\" % ((step + 1) * 64))\n",
        "\n",
        "  #Display metrics at the end each epoch.\n",
        "  train_acc =train_acc_metric.result()\n",
        "  print(\"epoch별 학습데이터 정확도: %.4f\" % (float(train_acc), ))\n",
        "\n",
        "  # Reset training metrics at the end of each epoch\n",
        "  train_acc_metric.reset_states()\n",
        "  \n",
        "  # Run a validation loop at the end of each epoch.\n",
        "  for x_batch_val, y_batch_val in val_dataset:\n",
        "    test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "  val_acc = val_acc_metric.result()\n",
        "  val_acc_metric.reset_states()\n",
        "  print(\"검증데이터 정확도 : %.4f\" % (float(val_acc), ))\n",
        "  print(\"걸린 시간: %.2fs\" % (time.time() - start_time))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0번째 epoch 시작\n",
            "Training loss (for one batch) at step 0 번째 스텝에서 배치별 훈련데이터 손실: 0.3174\n",
            "현재까지 학습한 데이터 개수: 64\n",
            "Training loss (for one batch) at step 200 번째 스텝에서 배치별 훈련데이터 손실: 0.5646\n",
            "현재까지 학습한 데이터 개수: 12864\n",
            "Training loss (for one batch) at step 400 번째 스텝에서 배치별 훈련데이터 손실: 0.1751\n",
            "현재까지 학습한 데이터 개수: 25664\n",
            "Training loss (for one batch) at step 600 번째 스텝에서 배치별 훈련데이터 손실: 0.4162\n",
            "현재까지 학습한 데이터 개수: 38464\n",
            "Training loss (for one batch) at step 800 번째 스텝에서 배치별 훈련데이터 손실: 0.4638\n",
            "현재까지 학습한 데이터 개수: 51264\n",
            "epoch별 학습데이터 정확도: 1.4870\n",
            "검증데이터 정확도 : 1.3589\n",
            "걸린 시간: 1.78s\n",
            "\n",
            "1번째 epoch 시작\n",
            "Training loss (for one batch) at step 0 번째 스텝에서 배치별 훈련데이터 손실: 0.1739\n",
            "현재까지 학습한 데이터 개수: 64\n",
            "Training loss (for one batch) at step 200 번째 스텝에서 배치별 훈련데이터 손실: 0.3054\n",
            "현재까지 학습한 데이터 개수: 12864\n",
            "Training loss (for one batch) at step 400 번째 스텝에서 배치별 훈련데이터 손실: 0.3826\n",
            "현재까지 학습한 데이터 개수: 25664\n",
            "Training loss (for one batch) at step 600 번째 스텝에서 배치별 훈련데이터 손실: 0.3235\n",
            "현재까지 학습한 데이터 개수: 38464\n",
            "Training loss (for one batch) at step 800 번째 스텝에서 배치별 훈련데이터 손실: 0.2700\n",
            "현재까지 학습한 데이터 개수: 51264\n",
            "epoch별 학습데이터 정확도: 1.3971\n",
            "검증데이터 정확도 : 1.2741\n",
            "걸린 시간: 1.65s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}